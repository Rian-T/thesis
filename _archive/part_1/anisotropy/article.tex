

Anisotropy has been widely observed among self-supervised models based on Transformers, and literature suggests that it may be a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens, as discussed in \Cref{chap:softmax_bottleneck}. However, this observation does not suffice to explain the anisotropy levels of other layers in pretrained language models, including those that seem to be affected less by these frequency-based distortions. For instance, the OPT models \citep{zhang2022opt} have isotropic last layers, but degenerated first layers (see \Cref{fig:opt_aniso} in \Cref{chap:softmax_bottleneck}).

This raises the question of the effect of anisotropy on the inner workings of Transformer layers, but also of the effect of the inner workings of Transformers layers on the geometry of their output distributions.


In this paper, we investigate the anisotropy problem in depth, and we make several contributions:
\begin{itemize}
    \item We demonstrate empirically that anisotropy can be observed in language models with character-aware architectures that should not suffer directly from the same consequences as token-based models. We extend our observations to Transformers trained on other modalities, such as image and audio data, and show that anisotropy cannot be explained solely based on linguistic properties;
    \item We provide empirical observations on the anisotropic properties of the Transformer block by studying untrained layers, and establish a relation between anisotropy and the general sharpness of the self-attention mechanism;
    \item We conduct an analysis of the representations used in self-attention (queries and keys) along training and show that anisotropy appears intrinsically in the self-attention mechanism, when training pushes for sharp patterns.
\end{itemize} 


\begin{figure}[ht]
    \centering
     \includegraphics[width=0.6\columnwidth]{sources/part_1/anisotropy/imgs/cosine_token.png}
     \caption{Average cosine-similarity between hidden representations across layers for token-level NLP models. For T5-base, we concatenate encoder and decoder results.}
     \label{fig:anisotropy_token}
\end{figure}

As mentioned in \Cref{ssec:degeneration}, several works have established a connection between word frequency and distortions of the latent spaces \citep{yu-etal-2022-rare, puccetti-etal-2022-outlier,rajaee-pilehvar-2022-isotropy}. \citet{bis-etal-2021-much} have shown that anisotropy in LMs could be explained by a global \textit{drift} of the representations in the same direction, thus unifying conclusions from \citet{ethayarajh-2019-contextual} and \citet{gao2018representation}. The authors propose that this drift is caused by the persistent updating of the representation of rare and unused tokens in a consistent direction, due to the nature of the softmax operation in the cross-entropy loss. They show that removing the average component to all representations leads to a nearly perfect isotropy.

Various methods have been proposed to reduce anisotropy in Transformer-based LMs at token-level \citep{rajaee-pilehvar-2021-cluster, Wang2020Improving}, or at sentence-level \citep{gao-etal-2021-simcse, yan-etal-2021-consert,su2021whiteningsentencerepresentationsbetter} (see \Cref{sec:rw_sent_embs}). They usually consist in post-processing the representations, and lead to downstream performance boosts. We argue that these positive results are paving the way for the search of pre-training objectives that do not introduce anisotropy in the first place, in the hope that the resulting models will also perform better without any post-processing, and potentially be trained more efficiently. This motivates us to gain a deeper understanding of the underlying factors that induce anisotropy, whether they belong in data, architectures, or training procedures.



\section{Anisotropy in pre-trained Transformers}
\subsection{Character-based NLP}
\label{sec:charbased}
\begin{figure}[ht]
    \centering
     \includegraphics[width=0.6\columnwidth]{sources/part_1/anisotropy/imgs/cosine_char_based.png}
     \caption{Average cosine-similarity between hidden representations across layers for character-level models.}
     \label{fig:cos_char_aware}
\end{figure}

To assert whether the cross-entropy objective applied on vocabularies containing rare tokens is the sole cause for the common drift issue, we explore anisotropy in character-based models. We study different architectures presented in \Cref{sec:tokfree}, and our character-level model:
\begin{itemize}
    \item CharacterBERT \citep{el-boukkouri-etal-2020-characterbert} is constructing whole word representations from character embeddings put through convolutions and highway layers, before feeding them to a Transformers architecture.
    \item CANINE \citep{clark-etal-2022-canine} is downsampling contextualized character representations via a strided convolution before feeding them to a Transformers. It can be trained either with a subword-based objective (CANINE-s) or with a character-level one (CANINE-c).
    \item MANTa-LM (see \Cref{chap:manta}) is based on a differentiable segmentation and embedding module added before an encoder-decoder model in the style of T5 \citep{2020t5}. It takes bytes as inputs and outputs, but builds internal representations that are usually based on several bytes.
    \item ByT5 \citep{xue-etal-2022-byt5} is a version of T5 that is trained at byte-level. To afford for more complex encoding, the authors resize the encoder-decoder architecture.
\end{itemize}

Neither of these architectures should suffer from out-of-vocabulary tokens in the process of creating representations. The models that predict at word or sub-word level (CharacterBERT and CANINE-s) could have the cross-entropy loss systematically pushing away rare item representations. However, it is rather unclear why this would imply an embedding drift for deeper layers. Hence, if anisotropy was only caused by the presence of unused or rare subwords, those character-level models should be much less prone to this issue.

To verify this hypothesis, we compute hidden representations for the validation set of the WikiText-103 corpus \citep{merity2017pointer}. We then compute the average cosine-similarity between two representations, uniformly taken in the whole validation corpus.

In fact, as shown in \Cref{fig:cos_char_aware}, those models all display significant levels of anisotropy in at least one of their layers. Interestingly, the models that are based solely on characters or bytes for input and prediction (ByT5, CANINE-c, and MANTA-LM) seem to display even higher levels of anisotropy. We note, as it is the case for the T5 model, that the ByT5 decoder displays extremely high levels of anisotropy.

\subsection{Other modalities}
\label{sec:other_mod}
\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.43\textwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/cosine_audio.png}
         \caption{Speech}
         \label{fig:cos_speech}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.43\textwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/cosine_vit_imagenet.png}
         \caption{Vision}
         \label{fig:cos_audio}
    \end{subfigure}
    \caption{Average cosine-similarity between hidden representations across layers for Speech and Vision modalities. We observe that across both modalities, several models display significant levels of anisotropy.}
    \label{fig:anisotropy_modalities}
\end{figure*}

We have shown in the previous section that character-level language models suffer from anisotropy similarly to token-level ones, hinting that subword token distributions are not solely responsible for anisotropy. Still, it may be argued that anisotropy is related to linguistic properties inherent to textual data. Thus, we proceed to explore the anisotropy problem for Transformers-based models in other modalities, specifically speech and vision.

For speech models, we consider wav2Vec 2.0 \citep{wav2vec}, HuBERT \citep{HuBERT}, and Whisper \citep{radford2022whisper} with the Common Voice 11.0 dataset \citep{commonvoice:2020}. For vision models, we use ViT \citep{Wu2020VisualTT}, BEiT \citep{beit-2021}, MiT \citep{segformer21}, and DEiT \citep{pmlr-v139-touvron21a} on the ImageNet dataset \citep{imagenet15russakovsky}.

As in \Cref{sec:charbased}, we infer hidden representations on the validation sets for each modality. We then uniformly sample pairs of vectors to get cosine-similarity values for every layer of every model. The averaged results are displayed in \Cref{fig:anisotropy_modalities}.

Once again, almost every model shows a significant level of anisotropy on some of its layers. Notably, speech models seem to have very anisotropic representations, as every layer of every model outputs an average cosine-similarity of at least $0.2$. We find some exceptions among vision models, since the MiT model seems to use isotropic representation spaces and the ViT model has a low average cosine-similarity for all its layers.

We also conduct the same experiment for convolution-based networks in the vision modality. The models at glance are ResNet \citep{he2016deep}, EfficientNet \citep{Tan2019EfficientNetRM}, CvT \citep{wu2021cvt}, ConvNeXt \citep{liu2022convnet}, and VAN \citep{guo2022visual}. For these networks, we flatten convolution maps to vectors before computing the cosine-similarity.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{sources/part_1/anisotropy/imgs/cosine_cnn_imagenet.png}
    \caption{Average cosine-similarity between hidden representations across layers for convolution-based vision models.}
    \label{fig:convbased}
\end{figure}

We observe in \Cref{fig:convbased} that most of the convolution-based models are isotropic. Interestingly, the only exception is ResNet-50, whose representations become more and more isotropic as one explores deeper layers. This could partially be explained by the fact that the batch normalization \citep{pmlr-v37-ioffe15} used in some of these models mitigates \textit{a posteriori} the drift effect by removing the mean component of the representations. However, the ConvNeXt model also seems to use isotropic representations while not using batch normalization, which shows that this is not the only factor in the isotropic behavior of these models.

\subsection{When is anisotropy ``high''?}

\begin{figure*}[ht]
     \centering
     \begin{subfigure}[b]{0.43\textwidth}
          \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/cosine_v_density.png}
          \caption{Density function of cosine-similarity}
          \label{fig:cosine_v_density}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.43\textwidth}
          \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/q95_dimension.png}
          \caption{95$^{\text{th}}$\! quartile of the cosine-similarity distribution}
          \label{fig:q95}
     \end{subfigure}
     \caption{Anisotropy metrics on multi-dimensional normal distributions as the dimension increases. In \Cref{fig:q95}, we add points for the average cosine-similarity level of Transformers models for several modalities.}
     \label{fig:anisotropy_high}
 \end{figure*}


It can be argued that describing anisotropy as the observation of ``high'' cosine-similarity values is not a convincing definition. This section aims at showing which ranges of cosine-similarity values are characteristic of anisotropic distributions. 
In \Cref{fig:cosine_v_density}, we show the density function of the cosine-similarity values obtained when drawing pairs of samples from isotropic normal distributions in $\mathbb{R}^d$ as $d$ increases. 

For smaller dimensions ($d=16$), we see that the range of cosine-similarity values that are reached between isotropic distributions is relatively broad compared to the possible spectrum ($[-1, 1]$). As $d$ increases, the support of the observed distributions seems to become smaller, due to the curse of dimensionality.

We analyze this effect more in-depth in \Cref{fig:q95}, where we plot the 95th quantile of the cosine-similarity distribution in the isotropic scenario. We also add values for the layer-wise average cosine-similarity levels of typical models in several modalities for comparison. We can clearly observe that the levels of cosine-similarity observed in the representations of Transformers-based models are significantly unlikely to be observed in between samples drawn in isotropic normal distributions.

Nevertheless, as we go towards higher dimensional spaces for bigger models (e.g. Llama-65B from \citet{touvron2023llama} has 8192 hidden dimensions), we believe that it may be relevant to introduce isotropy metrics that are grounded to isotropic cosine-similarity distributions. We leave this question for future works.

\subsection{To drift or not to drift?}
Related works \citep{bis-etal-2021-much, gao2018representation} show that anisotropy in subword-level language models is caused by a drift of the hidden representations in a shared direction. In this section, we try to extend this observation to other modalities.

We study the correlation between the uniformly measured cosine-similarity, and the norm of the average hidden representation $||\bar{h}||_2$ for each layer. If anisotropy could be directly explained by the drift effect, we would expect a monotonic relation between $||\bar{h}||_2$ and the average cosine-similarity. To verify this, we apply a Spearman correlation test on these two metrics for every model from \Cref{sec:charbased} and \Cref{sec:other_mod}, along with some token-level language models, namely T5 \citep{2020t5}, BERT \citep{devlin-etal-2019-bert}, RoBERTa \citep{roberta}, and GPT-2 \citep{gpt2}.

\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.43\columnwidth}
          \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/pval_vs_cosine_spearman.png}
          \subcaption{Spearman correlation test}
          \label{fig:pval_vs_cos_spearman}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.43\columnwidth}
          \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/pval_vs_cosine_pearson.png}
          \subcaption{Pearson correlation test}
          \label{fig:pval_vs_cos_pearson}
     \end{subfigure}
     \caption{p-value of correlation tests between the norm of the average representation and the cosine-similarity averaged over all layers, across modalities. For models above the red dotted line, there is no significant ($p>0.05$) correlation between the drift effect and the anisotropy level.}
     \label{fig:pval_vs_cos}
 \end{figure}


In \Cref{fig:pval_vs_cos}, we observe that we can correlate the anisotropy level and the magnitude of the drift component across layers for several models. The anisotropy of subword-based models can generally be correlated with the drift effect using the Spearman correlation test, except for GPT-2 for which it may not be appropriate.

The Pearson test measures a linear correlation between random variables, while the Spearman test measures a monotonic correlation. As there is no specific argument in favor of a linear relationship between the measured distributions (average cosine-similarity and norm of the average representation), we decide to favour the Spearman correlation test in order to take into account more complex relation patterns.

Nevertheless, this metric is based on the rank of each observation, and is thus not robust to fluctuations due to sample variance, specifically when working with such small samples. This is reflected by the discrepancy between Pearson and Spearman p-values for some models (e.g. GPT-2). Hence, we report both metrics for completeness.

Interestingly, we notice that the anisotropy affecting most CNN-based vision models is generally not correlated with the drift effect, contrary to Tranformers-based models in the same modality. Some speech models (HuBERT and Whisper-base) also display signs of anisotropy that cannot be correlated with the drift effect. \Cref{fig:pval_vs_cos} also shows a correlation for all character-based models but Canine-C and MANTa-base.

\section{Exploring the representation drift}
\label{sec:empirical}
% We've empirically explored the extent of the anisotropy effect in Transformers-based models, and showed that it affects various modalities.
In this section, we focus on some intrinsic properties of the Transformer block in a modality-agnostic fashion, i.e. with minimal assumptions on the data distribution, and without training. We analyze experimentally the behavior of the untrained Transformer block $T$ when a common bias term $b$ is added to untrained input representations $\mathbf{h}$. This allows us to mimic the common drift as mentioned in \citet{bis-etal-2021-much} and to identify some properties induced by this artificial drift on the output representations.

\subsection{Experimental setup}
We consider an embedding lookup table $E$ and a Transformer block $T$ with weights initialized as in BERT \citep{devlin-etal-2019-bert}. We then draw 16 input embedding sequences $\mathbf{h}$ of length 512 uniformly from $E$. To account for a drift component of norm $\eta \in\mathbb{R}$, we generate a vector $b_u \sim \mathcal{N}(0, I_d)$, which we normalize into $b_\eta = \frac{b_u}{||b_u||_2}\cdot \eta$. We finally compute $T(\mathbf{h} + b)$ for every sequence $\mathbf{h}$, and study the resulting distributions.

Specifically, we study the average norm of the input representations $\mathbb{E}(||\mathbf{h} + b_\eta||_2)$ against the average norm of the output representations $\mathbb{E}(||T(\mathbf{h} + b_\eta)||_2)$ in \Cref{fig:norm_scratch_transformer}. We also retrieve the self-attention scores before the softmax operation, namely $\frac{Q^h{K^h}^T}{\sqrt{d_k}}$, along with the corresponding $Q^h$ and $K^h$ matrices. We study some of their properties in \Cref{fig:attscore_trained_transformer} and \Cref{fig:kq}.

\subsection{Input vs. output analysis}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/scratch_bert_base_input_vs_output.png}
         \subcaption{Cosine similarity}
         \label{fig:cos_scratch_transformer}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/bert_base_norm_v_output.pdf}
         \subcaption{Norm}
         \label{fig:norm_scratch_transformer}
    \end{subfigure}
    \caption{Input/Output comparison of a Transformer block from BERT-base as the bias norms increases.}
    \label{fig:bias_vs_cosine_norm}
\end{figure}

In \Cref{fig:cos_scratch_transformer}, we observe that the output representations have an average cosine-similarity value that is slightly higher than the one of the input representations, no matter the level of input bias. We also notice that while the norm of the average output representation increases with the bias norm, it seems to meet the corresponding input measure for a given bias norm.

Interestingly, this shows that there is a \textit{fixed point} in terms of norm in the Transformers function with biased input. More formally, there seems to exist a bias norm $\eta^* \in \mathbb{R}_+$ such that: $$\mathbb{E}_{\mathbf{h}, b_{\eta^*}}(||\mathbf{h} + b_{\eta^*}||) = \mathbb{E}_{\mathbf{h}, b_{\eta^*}}(||T(\mathbf{h} + b_{\eta^*})||)$$

Moreover, this fixed point level $\eta^*$ is in the order of magnitude of the average hidden state norms of the layers of the trained BERT model. This hints that the model's representations stabilize when their norm is close to this fixed point. We leave a more thorough analysis of this hypothesis for future work.

\subsection{Exploring the Transformer block}

To understand the effect of the drift effect on the inner workings of the Transformer layer, we take a closer look at the self-attention operation as the average input representation drifts away.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{sources/part_1/anisotropy/imgs/trained_bert_base_att_scores.pdf}
    \caption{Histograms of the pre-softmax attention scores as the input bias norm increases. Other initializations of the layer and of the bias direction $b_u$ led to a general \textit{increase} of the attention scores instead.}
    \label{fig:attscore_trained_transformer}
\end{figure}

\Cref{fig:attscore_trained_transformer} shows that the attention scores tend to move away from zero as the input bias norm increases. Indeed, as the norm of the average $\bar{\mathbf{h}}$ of the input embeddings increases, we can expect the query and key vectors $Q^h$ and $K^h$ to also display signs of anisotropy. Actually, for each self-attention head, and for all position $i \in [1, L]$, we have:
\begin{equation}
    \begin{cases}
      \mathbb{E}_{\mathbf{h}}(Q^h_i) = W_{Q^h}\bar{\mathbf{h}} + b_{Q^h}\\
      \mathbb{E}_{\mathbf{h}}(K^h_i) = W_{K^h}\bar{\mathbf{h}} + b_{K^h}
    \end{cases}
\end{equation}

We can observe in \Cref{fig:kq} that query and key representations indeed increase in norm with the input bias norm. We also notice that the corresponding distributions are anisotropic even when no bias is added, which may be a consequence of BERT's initialization parameters.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/trained_bert_base_bias_vs_kq_cos.png}
         \caption{Cosine similarity}
         \label{fig:cos_qk_trained_transformer}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/trained_bert_base_bias_vs_kq_norm.png}
         \caption{Norm}
         \label{fig:norm_qk_trained_transformer}
    \end{subfigure}
    \caption{Analysis of the self-attention query and key distributions}
    \label{fig:kq}
\end{figure}

\subsection{Impact of the drift}

After exploring the consequences of the drift of input representations on the query-key product in self-attention, we identify in this section the implications of this drift at a more explainable level, by observing the resulting post-softmax distributions.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{sources/part_1/anisotropy/imgs/untrained_bert_base_bias_vs_softmax.png}
    \caption{Evolution of the self-attention softmax values as the input bias norm increases.}
    \label{fig:softmax_trained_transformer}
\end{figure}

In \Cref{fig:softmax_trained_transformer}, we retrieve softmax values in the self-attention block and for each position, we extract the maximum, the median and the minimum. We then average these values over the whole batch, and repeat for various input bias norm levels. We notice that as the input bias norm increases, the self-attention softmax distributions tend to become less entropic, evolving towards higher maximal probabilities and lower minimal probabilities. In the following analysis, we'll use the term \textit{sharpness} to discuss entropy levels of the self-attention distributions.
\FloatBarrier

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/untrained_bert_base_bias_vs_max_softmax.png}
         \caption{Maximum}
         \label{fig:max_softmax}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/untrained_bert_base_bias_vs_min_softmax.png}
         \caption{Minimum}
         \label{fig:min_softmax}
    \end{subfigure}
    \caption{Comparison of the extreme values of each sequence averaged over the batch as the bias norm increases.}
    \label{fig:min_vs_max}
\end{figure}

This sharpening effect of the attention distributions becomes even clearer if we consider the maximum and minimum values over the whole sequences, as in \Cref{fig:min_vs_max}.

However, at low anisotropy levels, i.e. when the bias norm is low, we see that the effect is not very important. \Cref{fig:softmax_trained_transformer} and \Cref{fig:min_vs_max} only hint at the fact that the drift of embeddings may help the self-attention to be sharper. Another explanation could be that training favors sharp self-attention patterns, as has been pointed out in previous works \citep{clark-etal-2019-bert}, which in turn induces a drift in the models' representations. In order to account for that, we need to study the evolution of latent spaces at the self-attention level along training.

\section{Queries and keys: training dynamics}
\label{sec:qk}

\subsection{Parallel drift of queries and keys}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.43\linewidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/dist_l9h9_s0.png}
         \caption{Step 0}
         \label{fig:dist_qk_s0}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\linewidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/dist_l9h9_s40.png}
         \caption{Step 40k}
         \label{fig:dist_qk_s40}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\linewidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/dist_l9h9_s200.png}
         \caption{Step 200k}
         \label{fig:dist_qk_s200}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\linewidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/dist_l9h9_s2000.png}
         \caption{Step 2M (final)}
         \label{fig:dist_qk_s2M}
    \end{subfigure}
    \caption{Evolution of $Q^h_s$ and $K^h_s$ distributions along training (on layer $9$ and head $h=9$). Vectors are projected using a common SVD.}
    \label{fig:proj_qk_heads}
\end{figure*}

We have established that manually pushing for drift-based anisotropy on \textit{untrained} Transformers models leads to sharper (i.e. low-entropy) self-attention patterns. In this section, we show that this evolution of self-attention values actually takes place during training, and we explore the mechanism behind their appearance. As pointed out in \Cref{sec:empirical}, the self-attention scores result from the $Q^h(K^h)^T$ operation, which computes scalar products between query and key representations corresponding to each pair of positions. Thus, in this section, we study the evolution of these query and key representations \textit{along training}, and explore the mechanism behind the increase of the scalar products leading to self-attention scores.

We use the MultiBERT checkpoints \citep{sellam2021multiberts} with seed 0 to retrieve $Q^h$ and $K^h$ distributions at different pretraining steps, and we use 128 samples from Wikitext-103 as input data. Along this section, $Q^h_s$ and $K^h_s$ refer to query and key representations extracted at a specific layer and head at a given step $s$, and $\hat{Q^h_s}$ and $\hat{K^h_s}$ are the average representations, taken over all tokens in the sampled batch. By studying $\bar{Q^h_s}$ and $\bar{K^h_s}$, we aim at exploring the common (or context-agnostic) drifts of keys and queries distributions.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/l0h3_samedir_QK.png}
         \caption{Similar}
         \label{fig:QK_simdir}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/l9h5_diffdir_QK.png}
         \caption{Opposite}
         \label{fig:QK_diffdir}
    \end{subfigure}
    \caption{Evolution of $\bar{Q^h_s}$ and $\bar{K^h_s}$ along training for two different heads in the network, projected via common SVD. Each arrow represents a checkpoint in the MultiBERT suite. We display typical examples of dynamics in same/opposite direction.}
    \label{fig:QK_dir}
\end{figure}

In \Cref{fig:proj_qk_heads} and \Cref{fig:QK_dir}, we compute a SVD of the union of $Q^h_s$ and $K^h_s$ for all steps $s$, so that the projection makes sense for both distributions across steps for visualization purposes \footnote{We actually uniformly sample 20\% of the whole set of representations to compute the SVD under reasonable memory constraints.}. As shown in our selected examples, we observe that the dynamics of $\bar{Q^h_s}$ and $\bar{K^h_s}$ tend to align along training, making the average of the distributions drift in either similar or opposite directions. The first dimension of the SVD seems to describe this common drift. Note that in $\mathbb{R}^{d_h}$ ($d_h = 64$ being the head dimension), such an alignment is very unlikely to happen randomly. Interestingly, \Cref{fig:QK_simdir} shows that the common direction dynamics appear in the first few steps, while the opposite direction dynamics of  \Cref{fig:QK_diffdir} only starts after 8\% of the total training steps.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.43\linewidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/l0_cosine_QK.png}
         \caption{Layer 0}
         \label{fig:cosine_qk_l0}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\linewidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/l4_cosine_QK.png}
         \caption{Layer 4}
         \label{fig:cosine_qk_l4}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\linewidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/l9_cosine_QK.png}
         \caption{Layer 9}
         \label{fig:cosine_qk_l9}
    \end{subfigure}
    \begin{subfigure}[b]{0.43\linewidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/l11_cosine_QK.png}
         \caption{Layer 11}
         \label{fig:cosine_qk_l11}
    \end{subfigure}
    \caption{Evolution of cosine-similarity between $\bar{Q^h_s}$ and $\bar{K^h_s}$ along training. Each color represents one self-attention head. Steps are counted in thousands. We generally observe that almost all heads see $\bar{Q^h_s}$ and $\bar{K^h_s}$ align in common or opposite directions along training. In other words, the average components of keys and queries representations tend to align in self-attention heads, which maximizes the magnitude of the scalar product between two average representations. We run a similar experiment on all MultiBERT seeds in \Cref{fig:seeds_qk}, and obtain comparable results.}
    \label{fig:cosine_qk_heads}
\end{figure*}
\FloatBarrier

To consolidate our observations, we compute the evolution of the cosine-similarity between $\bar{Q^h_s}$ and $\bar{K^h_s}$ along training in \Cref{fig:cosine_qk_heads}. We also display some projected $Q^h_s$ and $K^h_s$ distributions for several $s$ steps in \Cref{fig:proj_qk_heads}.

\Cref{fig:cosine_qk_heads} shows that the first layers display a common direction dynamic, as the cosine-similarity tends to increase, thus showing that \textbf{the key and query distributions drift along a similar direction} in average. The last layers seem to adopt an opposite direction dynamic, as the cosine-similarity between their mean key and query representations gets negative along training.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/l3h8_scalar_QK.png}
         \caption{Similar}
         \label{fig:scalar_sim}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.43\columnwidth}
         \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/l9h9_scalar_QK.png}
         \caption{Opposite}
         \label{fig:scalar_opp}
    \end{subfigure}
    \caption{Evolution of the scalar product between $\bar{Q^h_s}$ and $\bar{K^h_s}$ along training. Steps are in thousands.}
    \label{fig:scalar_QK}
\end{figure}

As shown in \Cref{fig:scalar_QK}, this drift induces an increase in the magnitude of scalar products obtained in the self-attention $Q^h{K^h}^T$ operation, thus facilitating the emergence of sharp patterns where attention focuses on specific tokens.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\linewidth]{sources/part_1/anisotropy/imgs/entropy_decay.png}
    \caption{Average entropy of the probability distributions corresponding to self-attention rows along training. Each curve corresponds to one layer.}
    \label{fig:entropy_decay}
\end{figure}

Finally, \Cref{fig:entropy_decay} describes the evolution of the average entropy in self-attention distributions. We observe that training induces an overall decay of the entropy for all layers, with different dynamics. This corresponds to sharper self-attention distributions. It is interesting to notice that the distributions in the first layers remain sharper than the ones in the last layers.

\subsection{Relation between sparsity and anisotropy}

Although there seems to be a clear correlation between parallel drift and sparsity in practice, we did not expose a clear causation between the coupled degeneration of queries and keys and the observed sparsity of attention maps. As a matter of fact, this parallel drift should only cause a bias in the attention values, which in turn has no incidence on the subsequent softmax-normalized attention map.

To better understand this phenomenon, we need to precisely characterize the structure of the sparsity in attention maps. In \Cref{fig:attn_maps}, we compute the attention maps of TinyLlama \citep{tinyllama}, a 1B parameter causal language model based on the Llama suite \citep{touvron2023llama}.

\begin{figure}[!ht]
\centering
\begin{subfigure}[b]{0.45\columnwidth}
     \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/qk_position_l2h5.png}
     \caption{Layer 3, head 6}
     \label{fig:l2h5}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\columnwidth}
     \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/qk_position_l12h5.png}
     \caption{Layer 13, head 6}
     \label{fig:l12h5}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\columnwidth}
     \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/qk_position_l20h5.png}
     \caption{Layer 21, head 6}
     \label{fig:l20h5}
\end{subfigure}
\caption{TinyLlama's attention maps for different layers and heads.}
\label{fig:attn_maps}
\end{figure}

As shown in \Cref{fig:attn_maps} attention maps are mostly sparse, following patterns where self-interaction dominate (\Cref{fig:l2h5}), where interactions are mostly active in a short context window before the current position (\Cref{fig:l12h5}), or where some positions are attended by all future ones (\Cref{fig:l20h5}). 

We start by anayzing this latter case through the anisotropy lens. In \Cref{fig:attn_maps_l1h8}, we measure cosine-similarity between the average query representation $\bar{Q^h}$ and the key representations $K^h_t$, for an attention head that behaves accordingly.

\begin{figure}[!ht]
\centering
\begin{subfigure}[b]{0.45\columnwidth}
     \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/attn_l1h8.png}
     \caption{Attention map}
     \label{fig:amap_l1h8}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\columnwidth}
     \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/cosine_q_khat_l1h8.png}
     \caption{Cosine-similarity of keys and the average query representations}
     \label{fig:cosim_l1h8}
\end{subfigure}
\caption{Analysis of TinyLlama's attention map for layer 2 and head 9.}
\label{fig:attn_maps_l1h8}
\end{figure}

\Cref{fig:attn_maps_l1h8} illustrates how the cosine-similarity of keys with the average query component allows to differentiate between attended and ignored tokens. In other words, the norm of the projections of key vectors $K^h_t$ in the common drift direction represented by $\bar{Q^h}$ allows to separate between discardable positions and important ones. Most interactions being ignored by the attention pattern, we observe that most of the measured cosine-similarities are negative.

For attention patterns that resemble \Cref{fig:l12h5}, we need to use a different approach as the attention value seems to depend on both key and query indices. In \Cref{fig:attn_maps_l12h5}, we propose to select a several fixed query indices and to measure the cosine-similarity with past keys.

\begin{figure}[!ht]
\centering
\begin{subfigure}[b]{0.45\columnwidth}
     \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/qk_position_l12h5.png}
     \caption{Attention map}
     \label{fig:amap_l12h5}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\columnwidth}
     \includegraphics[width=\linewidth]{sources/part_1/anisotropy/imgs/cosine_q_khat_l12h5.png}
     \caption{Cosine-similarity of queries and keys}
     \label{fig:cossim_l12h5}
\end{subfigure}
\caption{Analysis of TinyLlama's attention map for layer 13 and head 6.}
\label{fig:attn_maps_l12h5}
\end{figure}

In \Cref{fig:attn_maps_l12h5}, we remark a similar pattern than in \Cref{fig:attn_maps_l1h8}, where all interactions correspond to negative cosine-similarity, and where dominant interactions correspond to higher cosine-similarity levels. As most interactions are apparently discarded in this attention pattern, $Q^h_t$ representations need to have negative cosine-similarity with an increasing number of $K^h_t$ as we move further into the sequence, which reinforces an overall alignment of these representations.

Hence, the parallel drift of queries and keys representations facilitates the emergence of a common direction that encodes a selective process in the self-attention patterns, where discardable interactions lead to lower cosine-similarity values. 

Overall, this section shows that drift anisotropy emerges in the query and key representations during the training of MultiBERT, as self-attention distributions become sharper. The drifts of queries and keys tend to align, thus increasing the magnitude of scalar products, and the general sharpness of self-attention.

Although this section focuses on the case of token-based NLP, we believe that strong attention patterns may be required when training Transformers across all modalities, potentially generating distortions in query and key distributions that account for the final observed anisotropy of the models. However, we could not extend experiments to other modalities due to the lack of released intermediate checkpoints, to the best of our knowledge.

\section{Discussion}
\label{sec:anisotropy_discussion}

In this work, we argue that the nature of data distributions is not solely responsible for the anisotropy observed in most hidden representations of Transformers-based models across modalities. As \Cref{sec:empirical} shows, untrained Transformers layers display a tendency towards anisotropy. Biased inputs tend to increase the variance of the attention scores and thus facilitate the emergence of sharp patterns in the self-attention mechanisms. We also show in \Cref{sec:qk} that along training, query and key distributions drift in parallel directions, which increases anisotropy in the inner representations of the Transformer layers, while allowing sharper attention patterns. As discussed in \citet{puccetti-etal-2022-outlier}, outlier dimensions in Transformers are also involved in the emergence of strong attention patterns.

\paragraph{Consistency of the SVD} In \Cref{sec:qk}, we use an SVD on the \textit{union} of $Q^h_s$ and $K^h_s$ for visualization purposes (see \Cref{fig:proj_qk_heads} and \Cref{fig:QK_dir}). It may be argued that this approach favors the emergence of a discriminative singular direction, that helps distinguish between keys and queries, thus supporting the findings in a less convincing way. To address this concern, we display alternative projections in \Cref{sec:other_projs}, where we compute the SVD on $Q^h_s$ or $K^h_s$ only, and then project all representations using this SVD. Our observations show that our findings are consistent for these alternative projections.

\paragraph{Harmfulness of anisotropy} Even though anisotropy has not been shown to be an issue in language modeling, previous works have advocated that removing anisotropy in output representations leads to better sense disambiguation abilities \citep{bihani-rayz-2021-low, bis-etal-2021-much}. Isotropic models could also improve cross-lingual alignment in multilingual language models \citep{h√§mmerl2023exploring}. Nevertheless, concurrent works have suggested that anisotropy may not hurt the quality of the representations \citep{ait-saada-nadif-2023-anisotropy, rudman2023stable}. We argue that anisotropy in the Transformer architecture may actually help models by allowing sharp attention patterns, but we also believe that our work can pave the way for new architectures that can easily use sharp attention patterns without inducing anisotropy. 


\section*{Conclusion}
In this paper, we investigated the anisotropy problem through the lens of the drift effect, and made several contributions to the understanding of this phenomenon. We demonstrated that anisotropy can be observed in language models with character-aware architectures, extended our observations to Transformers trained on other modalities, and studied anisotropy in untrained Transformers layers. We finally explored the training dynamics of the query and key distributions, and found that they drift along a shared direction hence maximizing $Q^h{K^h}^T$ scalar products in absolute value, allowing stronger attention patterns as a result.

We conclude that anisotropy almost systematically affects Transformers on all modalities, in a way that is not always correlated with the drift of the representations. We also provide empirical evidence that anisotropy appears as an inherent property of latent distributions used in the self-attention mechanism when modeling sharp attention patterns. We hypothesize that a revision of the self-attention operation could help reduce anisotropy by facilitating the emergence of sharp attention softmax distributions without distorting the geometry of the hidden representations.


\section*{Limitations}
As mentioned in \Cref{sec:anisotropy_discussion}, we acknowledge that \Cref{sec:empirical} does not take into account the training dynamics, and only exposes some properties of the Transformer layer at initialization.

Moreover, we are aware that our approach is not theoretically rigorous in some aspects. For instance, we don't prove that sharp self-attention patterns \textit{cannot} emerge without anisotropy in keys and queries representations. In other words, this article is focusing on exposing and \textit{correlating} factors that explain anisotropy, but we do not demonstrate theoretical properties that would help identify the \textit{causes} of anisotropy. Nevertheless, we believe that our work can pave the way for such theoretical exploration in the future.

\vspace{2em}

In this section, we show that representation degeneration happens in the self-attention layers and co-occurs with the sparsification of attention patterns, regardless of the data modality. This incentivizes the analysis of representation geometry as a way to better understand these implicit biases and paths towards how to improve them. 


% \section*{Ethics Statement}
% To the best of our knowledge, our work does not raise any ethical concern. However, as noted in \citet{zhou2021freqbased}, we believe that distortions in the embedding space may be related to bias in the training data, whether it is inherent to the structure of the modality (e.g. the Zipfian distribution of words), or due to human factors (e.g. geographical considerations).

